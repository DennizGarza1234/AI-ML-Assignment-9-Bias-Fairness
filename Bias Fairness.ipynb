{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c00d1d82",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG), Foundation Model Selection, and Temperature Control\n",
    "\n",
    "This notebook covers three major interview topics related to Large Language Models (LLMs):\n",
    "\n",
    "1. **Retrieval-Augmented Generation (RAG)**\n",
    "2. **Choosing the Right Foundation Model**\n",
    "3. **Impact of the Temperature Parameter (with STAR example)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076c2699",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG)\n",
    "**Concept:**\n",
    "Retrieval-Augmented Generation (RAG) combines *retrieval* (finding relevant documents) with *generation* (LLM text creation).  \n",
    "It helps LLMs stay accurate, current, and domain-aware without full retraining.\n",
    "markdown\n",
    "**Why RAG is Needed:**\n",
    "- Prevents **hallucination** by grounding answers in real data.  \n",
    "- Keeps knowledge **up-to-date** via document indexing instead of retraining.  \n",
    "- Enables **domain adaptation** for specialized fields (medical, legal, etc.).  \n",
    "- Improves **efficiency** by reducing the required model size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc62d368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Air pollution leads to respiratory issues and heart disease.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def mock_llm_response(prompt, temperature=0.3):\n",
    "    \"\"\"Simulates different outputs based on temperature.\"\"\"\n",
    "    deterministic_answer = \"Air pollution leads to respiratory issues and heart disease.\"\n",
    "    creative_variations = [\n",
    "        \"Air pollution silently steals our breath and burdens the heart.\",\n",
    "        \"Dirty air harms lungs, hearts, and our shared environment.\",\n",
    "        \"Polluted skies mean unhealthy lungs and heavy hearts.\"\n",
    "    ]\n",
    "    if temperature < 0.4:\n",
    "        return deterministic_answer\n",
    "    else:\n",
    "        return random.choice(creative_variations)\n",
    "\n",
    "knowledge_base = {\n",
    "    \"air pollution\": \"Air pollution causes respiratory issues and heart disease.\",\n",
    "    \"climate change\": \"Climate change increases the frequency of severe weather events.\",\n",
    "}\n",
    "\n",
    "def retrieve_docs(query):\n",
    "    for key, text in knowledge_base.items():\n",
    "        if key in query.lower():\n",
    "            return text\n",
    "    return \"No relevant documents found.\"\n",
    "\n",
    "def rag_generate(query, temperature=0.3):\n",
    "    context = retrieve_docs(query)\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    return mock_llm_response(prompt, temperature)\n",
    "\n",
    "query = \"What are the health impacts of air pollution?\"\n",
    "print(rag_generate(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab22cddb",
   "metadata": {},
   "source": [
    "## Choosing the Right Foundation Model\n",
    "Selecting the right foundation model depends on:\n",
    "1. **Task Type** – generation, summarization, reasoning, etc.  \n",
    "2. **Domain Needs** – medical, legal, coding, creative, etc.  \n",
    "3. **Performance Metrics** – accuracy, latency, token cost, etc.  \n",
    "4. **Fine-tuning or RAG options** – can it be customized?\n",
    "\n",
    "| Application | Recommended Model | Reason |\n",
    "|--------------|-------------------|--------|\n",
    "| Summarization | GPT-4, Claude, or Llama 3 | Strong in natural text compression and contextual understanding |\n",
    "| Code Generation | Codex, CodeLlama, StarCoder | Trained specifically on large-scale code corpora |\n",
    "| Multimodal (text + image) | Gemini, GPT-4V | Integrates visual and textual reasoning |\n",
    "| Domain-Specific (e.g., Legal, Medical) | Fine-tuned LLaMA or Mistral | Customizable for specialized knowledge |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e45c738e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended model: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "def select_model(task):\n",
    "    if task == \"summarization\":\n",
    "        return \"gpt-4o-mini\"\n",
    "    elif task == \"code\":\n",
    "        return \"gpt-4o-code\"\n",
    "    elif task == \"multimodal\":\n",
    "        return \"gpt-4o-vision\"\n",
    "    else:\n",
    "        return \"llama3-finetuned\"\n",
    "\n",
    "print(\"Recommended model:\", select_model(\"summarization\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68068c8",
   "metadata": {},
   "source": [
    "## Impact of the Temperature Parameter\n",
    "**Temperature** controls randomness in model responses:\n",
    "- **Low temperature (0.1–0.3):** Focused, factual, and deterministic.\n",
    "- **High temperature (0.7–1.0):** Creative, diverse, and exploratory.\n",
    "\n",
    "Let's demonstrate this in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4081d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Temperature 0.2 ---\n",
      "Reliable rescue, ready for any storm.\n",
      "\n",
      "--- Temperature 0.6 ---\n",
      "Wings of hope through every storm.\n",
      "\n",
      "--- Temperature 0.9 ---\n",
      "Wings of hope through every storm.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "prompt = \"Write a short tagline for a disaster response aircraft.\"\n",
    "\n",
    "def mock_temperature_response(prompt, temperature):\n",
    "    low_temp_response = \"Reliable rescue, ready for any storm.\"\n",
    "    creative_variations = [\n",
    "        \"Wings of hope through every storm.\",\n",
    "        \"Braving chaos to bring calm.\",\n",
    "        \"When disaster strikes, we rise.\"\n",
    "    ]\n",
    "    if temperature < 0.4:\n",
    "        return low_temp_response\n",
    "    else:\n",
    "        return random.choice(creative_variations)\n",
    "\n",
    "for temp in [0.2, 0.6, 0.9]:\n",
    "    print(f\"\\n--- Temperature {temp} ---\")\n",
    "    print(mock_temperature_response(prompt, temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456f98d6",
   "metadata": {},
   "source": [
    "### STAR Example (Situation–Task–Action–Result)\n",
    "**Situation:**  \n",
    "During a humanitarian operations project, LLM-generated mission summaries were inconsistent in tone and clarity.  \n",
    "\n",
    "**Task:**  \n",
    "We needed to produce reliable, factual mission reports.  \n",
    "\n",
    "**Action:**  \n",
    "Lowered temperature from 0.8 → 0.2 in the generation pipeline to reduce randomness.  \n",
    "\n",
    "**Result:**  \n",
    "Summaries became consistent, accurate, and professional — improving reporting efficiency by 35%.\n",
    "# Summary\n",
    "- **RAG**: Combines retrieval + generation for factual grounding.  \n",
    "- **Model Choice**: Depends on task, domain, and compute trade-offs.  \n",
    "- **Temperature**: Adjusts creativity and precision balance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
